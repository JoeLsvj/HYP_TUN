{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ES for ht for classification \n",
    "\n",
    "HT to tune \n",
    "- lr \n",
    "- dropout \n",
    "- number of units \n",
    "- activation function \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libs\n",
    "\n",
    "import torch # the main pytorch library\n",
    "from torch import nn # the sub-library containing neural networks\n",
    "from torch.utils.data import DataLoader, Subset # an object that generates batches of data for training/testing\n",
    "from torchvision import datasets # popular datasets, architectures and common image transformations\n",
    "from torchvision.transforms import ToTensor # an easy way to convert PIL images to Tensors\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np # NumPy, the python array library\n",
    "import random # for generating random numbers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Custom modules\n",
    "import Models_class as Models\n",
    "import Dataset\n",
    "import utils\n",
    "\n",
    "# Set seeds\n",
    "\n",
    "SEED = 0 # seed for reproducibility\n",
    "\n",
    "random.seed(SEED) # set the seed for random numbers\n",
    "np.random.seed(SEED) # set the seed for numpy arrays\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # use the GPU if available\n",
    "#print(f\"default device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../../../Data/FI-2010-Cleaned/NoAuction/Zscore/WRT1V.parquet')\n",
    "df = df.head(500) # For testing purposes (put 20000)\n",
    "df.drop(columns=['Day'], inplace=True)\n",
    "\n",
    "target_cols = ['Label_10', 'Label_20', 'Label_30', 'Label_50', 'Label_100']\n",
    "# Set target cols to dtype int\n",
    "df[target_cols] = df[target_cols].astype(int)\n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreturn = lambda x: np.log(x) - np.log(x.shift(1))\n",
    "\n",
    "price_cols = [col for col in df.columns if 'P_' in col]\n",
    "df[price_cols] = df[price_cols].apply(logreturn, axis=0)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into train and test\n",
    "df_train, df_val, df_test = utils.DataTools.train_val_test_split(df)\n",
    "\n",
    "# Split covariates and response variable(s)\n",
    "X_train, y_train = utils.DataTools.split_x_y(df_train, target_cols)\n",
    "X_val, y_val     = utils.DataTools.split_x_y(df_val, target_cols)\n",
    "X_test, y_test   = utils.DataTools.split_x_y(df_test, target_cols)\n",
    "\n",
    "# Standardize the covariates\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_X = StandardScaler()\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_val = scaler_X.transform(X_val)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "# Build tensors \n",
    "X_train, X_val, X_test = utils.DataTools.numpy_to_tensor( X_train, X_val, X_test, device=device, dtype=torch.float32 )\n",
    "y_train, y_val, y_test = utils.DataTools.numpy_to_tensor( y_train, y_val, y_test, device=device, dtype=torch.long )\n",
    "\n",
    "# Build TimeSeriesDataset\n",
    "SEQ_LEN_X = 50\n",
    "SEQ_LEN_Y = 1\n",
    "OFFSET = 0\n",
    "dataset_train = Dataset.TimeSeriesDataset(\n",
    "    X         = X_train,\n",
    "    y         = y_train,\n",
    "    seq_len_x = SEQ_LEN_X,\n",
    "    seq_len_y = SEQ_LEN_Y,\n",
    "    offset    = OFFSET,\n",
    "    channels  = False,\n",
    "    task      = Dataset.TimeSeriesDataset.Task.CLASSIFICATION\n",
    ")\n",
    "dataset_val = Dataset.TimeSeriesDataset(\n",
    "    X         = X_val,\n",
    "    y         = y_val,\n",
    "    seq_len_x = SEQ_LEN_X,\n",
    "    seq_len_y = SEQ_LEN_Y,\n",
    "    offset    = OFFSET,\n",
    "    channels  = False,\n",
    "    task      = Dataset.TimeSeriesDataset.Task.CLASSIFICATION\n",
    ")\n",
    "dataset_test = Dataset.TimeSeriesDataset(\n",
    "    X         = X_test,\n",
    "    y         = y_test,\n",
    "    seq_len_x = SEQ_LEN_X,\n",
    "    seq_len_y = SEQ_LEN_Y,\n",
    "    offset    = OFFSET,\n",
    "    channels  = False,\n",
    "    task      = Dataset.TimeSeriesDataset.Task.CLASSIFICATION\n",
    ")\n",
    "\n",
    "# Build TimeSeriesLoader\n",
    "BATCH_SIZE = 32\n",
    "dataloader_train = Dataset.TimeSeriesLoader(\n",
    "    dataset = dataset_train,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True\n",
    ")\n",
    "dataloader_val = Dataset.TimeSeriesLoader(\n",
    "    dataset = dataset_val,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False\n",
    ")\n",
    "dataloader_test = Dataset.TimeSeriesLoader(\n",
    "    dataset = dataset_test,\n",
    "    batch_size = 1024,\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful commands \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The EA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import es_classification_functions as esc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac7989893084218b9ea4cb61f574d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce32066f0eb4d638551156c6ff130c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated time: 0 hours and 1 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf01e91d1184e10bf8d8468667899f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26bc0284d6a4d45971f0e5348597860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370aa44aa3c84aefb9fa8b7bfc385b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage\n",
    "max_fitness_evaluations = 5 # Just for trial\n",
    "num_epochs_per_nn_evaluation = 1\n",
    "\n",
    "mu_population_size = 2\n",
    "init_layersNumber_interval = [1,1]\n",
    "init_unitsNumber_interval = [10,1000]\n",
    "init_activation_functions = [nn.ReLU] \n",
    "init_lr_interval = (0.0001, 0.1)\n",
    "init_dropout_interval = (0.0, 0.2)\n",
    "nn_training_data = dataloader_train\n",
    "nn_validation_data = dataloader_val\n",
    "nn_input_size = 40\n",
    "nn_output_size = 15\n",
    "device = 'cpu' # 'cuda'\n",
    "maximize_fitness = False\n",
    "lambda_offsprings_size = 3\n",
    "tornamentPercentage_for_offspringGeneration = 0.2\n",
    "p_crossover_vs_mutation = 0.75\n",
    "structure_k_value_for_crossover = 0\n",
    "structure_lb_size = 10\n",
    "structure_ub_size = np.inf\n",
    "lr_k = 0\n",
    "dropout_k = 0\n",
    "lr_lb = 0.0001\n",
    "lr_ub = 0.1\n",
    "dropout_lb = 0.0\n",
    "dropout_ub = 0.2\n",
    "csv_file_path = './csv_results/es_classification1.csv'\n",
    "\n",
    "best_individual_ever = esc.es_lambdaMu_sld_classification(\n",
    "    max_fitness_evaluations = max_fitness_evaluations,\n",
    "    mu_population_size = mu_population_size,\n",
    "    init_layersNumber_interval = init_layersNumber_interval,\n",
    "    init_unitsNumber_interval = init_unitsNumber_interval,\n",
    "    init_activation_functions = init_activation_functions,\n",
    "    init_lr_interval = init_lr_interval,\n",
    "    init_dropout_interval = init_dropout_interval,\n",
    "    nn_training_data = nn_training_data,\n",
    "    nn_validation_data = nn_validation_data,\n",
    "    num_epochs_per_nn_evaluation = num_epochs_per_nn_evaluation,\n",
    "    nn_input_size = nn_input_size,\n",
    "    nn_output_size = nn_output_size,\n",
    "    device = device,\n",
    "    maximize_fitness = maximize_fitness,\n",
    "    lambda_offsprings_size = lambda_offsprings_size,\n",
    "    tornamentPercentage_for_offspringGeneration = tornamentPercentage_for_offspringGeneration,\n",
    "    p_crossover_vs_mutation = p_crossover_vs_mutation,\n",
    "    structure_k_value_for_crossover = structure_k_value_for_crossover,\n",
    "    structure_lb_size = structure_lb_size,\n",
    "    structure_ub_size = structure_ub_size,\n",
    "    lr_k = lr_k,\n",
    "    dropout_k = dropout_k,\n",
    "    lr_lb = lr_lb,\n",
    "    lr_ub = lr_ub,\n",
    "    dropout_lb = dropout_lb,\n",
    "    dropout_ub = dropout_ub,\n",
    "    csv_file_path=csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
